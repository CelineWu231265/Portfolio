{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A/B Test Analysis\n",
    "We're going to conduct an Independent Samples T-test to analyse our A/B test. An Indepdent Samples T-test compares the differences between two means of two different samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export your results to a .csv file and save it to you github repository. Import your .csv file, inspect it, and clean it where neccesary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16 entries, 0 to 15\n",
      "Data columns (total 6 columns):\n",
      " #   Column                                      Non-Null Count  Dtype\n",
      "---  ------                                      --------------  -----\n",
      " 0   Id                                          16 non-null     int64\n",
      " 1   I understood what I could use the app for.  16 non-null     int64\n",
      " 2   I found the application intuitive to use.   16 non-null     int64\n",
      " 3   I thought the application was useful.       16 non-null     int64\n",
      " 4   I enjoyed using the application.            16 non-null     int64\n",
      " 5   The app flow was self explanatory           16 non-null     int64\n",
      "dtypes: int64(6)\n",
      "memory usage: 896.0 bytes\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23 entries, 0 to 22\n",
      "Data columns (total 6 columns):\n",
      " #   Column                                      Non-Null Count  Dtype\n",
      "---  ------                                      --------------  -----\n",
      " 0   Id                                          23 non-null     int64\n",
      " 1   I understood what I could use the app for.  23 non-null     int64\n",
      " 2   I found the application intuitive to use.   23 non-null     int64\n",
      " 3   I thought the application was useful.       23 non-null     int64\n",
      " 4   I enjoyed using the application.            23 non-null     int64\n",
      " 5   The app flow was self explanatory           23 non-null     int64\n",
      "dtypes: int64(6)\n",
      "memory usage: 1.2 KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>I understood what I could use the app for.</th>\n",
       "      <th>I found the application intuitive to use.</th>\n",
       "      <th>I thought the application was useful.</th>\n",
       "      <th>I enjoyed using the application.</th>\n",
       "      <th>The app flow was self explanatory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  I understood what I could use the app for.  \\\n",
       "0   1                                           1   \n",
       "1   2                                           6   \n",
       "2   3                                           5   \n",
       "3   4                                           6   \n",
       "4   5                                           7   \n",
       "\n",
       "   I found the application intuitive to use.  \\\n",
       "0                                          1   \n",
       "1                                          1   \n",
       "2                                          5   \n",
       "3                                          5   \n",
       "4                                          7   \n",
       "\n",
       "   I thought the application was useful.  I enjoyed using the application.  \\\n",
       "0                                      1                                 1   \n",
       "1                                      1                                 1   \n",
       "2                                      5                                 5   \n",
       "3                                      5                                 5   \n",
       "4                                      7                                 7   \n",
       "\n",
       "   The app flow was self explanatory   \n",
       "0                                   1  \n",
       "1                                   1  \n",
       "2                                   5  \n",
       "3                                   6  \n",
       "4                                   7  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If on Github, load your data\n",
    "df_A = pd.read_csv(\"/Users/celinewu/Documents/GitHub/2023-24c-fai1-adsai-CelineWu231265/AB_test/Version_A - AB-test_HCAI_Version_A(Sheet1).csv\")\n",
    "df_B = pd.read_csv(\"/Users/celinewu/Documents/GitHub/2023-24c-fai1-adsai-CelineWu231265/AB_test/Version_B - AB-test_HCAI_Version_B(Sheet1).csv\")\n",
    "\n",
    "# EDA A\n",
    "df_A.info() # Is your data in the right format?\n",
    "df_A.head() # Quick EDA. No? Clean it, you only want the rows and columns containing likert-score data, saved as integers.\n",
    "\n",
    "# EDA B\n",
    "df_B.info() # Is your data in the right format?\n",
    "df_B.head() # Quick EDA. No? Clean it, you only want the rows and columns containing likert-score data, saved as integers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest we leave for tomorrow when we actually have our data. But if you are eager to play around a bit you can simply refresh the survey and fill in a couple of responses to create an A and a B version.\n",
    "\n",
    "Now, let's start analysing our gathered data! This block we won't dive into inferential statistics since it can get complex quite fast; we'll do that in Year 2, block A. For now, you just need to know that we need to test whether the data is normally distributed and whether the variances of both samples are equal. Otherwise, our statistical tests would not be valid and we can therefore not say that the results we're seeing are due to chance. What we are going to statistically ascertain is whether there is a statistically significant different in the mean of a given variable for version A or B. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the p-value is above 0.05, then the data is normally distrubted ShapiroResult(statistic=0.7951330258826621, pvalue=0.0023486318428322956) . If the data is not normally distributed then you will have to run the bootstrapped version.\n",
      "If the p-value is above 0.05, then the data is normally distrubted ShapiroResult(statistic=0.6664171981299241, pvalue=5.290371493857222e-06) . If the data is not normally distributed then you will have to run the bootstrapped version.\n",
      "If the p-value is above 0.05, then the groups have equal variances LeveneResult(statistic=0.3357001972386588, pvalue=0.5658330056280033) . If the variance aren't equal then you will have to run the bootstrapped version.\n"
     ]
    }
   ],
   "source": [
    "# Run the shaprio-wilk statistical test for each question to check whether the data is normally distributed\n",
    "normal_a = stats.shapiro(df_A[\"I understood what I could use the app for.\"])\n",
    "normal_b = stats.shapiro(df_B[\"I understood what I could use the app for.\"])\n",
    "\n",
    "# Check whether the variance of both samples is equal\n",
    "homogeneity = stats.levene(df_A[\"I understood what I could use the app for.\"],\n",
    "                           df_B[\"I understood what I could use the app for.\"])\n",
    "\n",
    "print(f\"If the p-value is above 0.05, then the data is normally distrubted\", normal_a, \". If the data is not normally distributed then you will have to run the bootstrapped version.\")\n",
    "print(f\"If the p-value is above 0.05, then the data is normally distrubted\", normal_b, \". If the data is not normally distributed then you will have to run the bootstrapped version.\")\n",
    "print(f\"If the p-value is above 0.05, then the groups have equal variances\", homogeneity, \". If the variance aren't equal then you will have to run the bootstrapped version.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that is in the right format and we know the column names. Replace 'A' with the column name which holds your original baseline version; A. Replace 'B' with the column name which holds the result of your improved version; B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results are significant if the p-value is significant, which means smaller than 0.05 TtestResult(statistic=-0.4951507864823368, pvalue=0.6234215810963804, df=37.0) \n",
      " If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\n"
     ]
    }
   ],
   "source": [
    "# Run Independent Samples T-test when assumptions are not violated.\n",
    "results = stats.ttest_ind(df_A[\"I understood what I could use the app for.\"],\n",
    "                          df_B[\"I understood what I could use the app for.\"])\n",
    "\n",
    "# Print the results\n",
    "print(f\"The results are significant if the p-value is significant, which means smaller than 0.05\", \n",
    "results,\n",
    "\"\\n\", \n",
    "\"If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results are significant if the p-value is significant, which means => 0.05 TtestResult(statistic=-0.4951507864823368, pvalue=0.6234215810963804, df=37.0) \n",
      " If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\n"
     ]
    }
   ],
   "source": [
    "# Run Bootstrapped Independent Samples T-test when assumptions are violated\n",
    "rng = np.random.default_rng() # create random sampling\n",
    "\n",
    "results = stats.ttest_ind(df_A[\"I understood what I could use the app for.\"],\n",
    "                          df_B[\"I understood what I could use the app for.\"],\n",
    "                          random_state = rng)\n",
    "\n",
    "# Print the results\n",
    "print(f\"The results are significant if the p-value is significant, which means => 0.05\", \n",
    "results,\n",
    "\"\\n\", \n",
    "\"If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, that was our first t-test. Save the results to your learning log in the week 8 and interpret them there. Were they what you expected? What are you going to change to improve your design if neccesary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the p-value is above 0.05, then the data is normally distrubted ShapiroResult(statistic=0.7773626318229307, pvalue=0.001384357498053207) . If the data is not normally distributed then you will have to run the bootstrapped version.\n",
      "If the p-value is above 0.05, then the data is normally distrubted ShapiroResult(statistic=0.7159623485779187, pvalue=2.2712078213567356e-05) . If the data is not normally distributed then you will have to run the bootstrapped version.\n",
      "If the p-value is above 0.05, then the groups have equal variances LeveneResult(statistic=0.8079341284946717, pvalue=0.37454430351244306) . If the variance aren't equal then you will have to run the bootstrapped version.\n"
     ]
    }
   ],
   "source": [
    "# Run the shaprio-wilk statistical test for each question to check whether the data is normally distributed\n",
    "normal_a = stats.shapiro(df_A[\"I found the application intuitive to use.\"])\n",
    "normal_b = stats.shapiro(df_B[\"I found the application intuitive to use.\"])\n",
    "\n",
    "# Check whether the variance of both samples is equal\n",
    "homogeneity = stats.levene(df_A[\"I found the application intuitive to use.\"],\n",
    "                           df_B[\"I found the application intuitive to use.\"])\n",
    "\n",
    "print(f\"If the p-value is above 0.05, then the data is normally distrubted\", normal_a, \". If the data is not normally distributed then you will have to run the bootstrapped version.\")\n",
    "print(f\"If the p-value is above 0.05, then the data is normally distrubted\", normal_b, \". If the data is not normally distributed then you will have to run the bootstrapped version.\")\n",
    "print(f\"If the p-value is above 0.05, then the groups have equal variances\", homogeneity, \". If the variance aren't equal then you will have to run the bootstrapped version.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results are significant if the p-value is significant, which means smaller than 0.05 TtestResult(statistic=-1.0472272162506624, pvalue=0.3017879146791575, df=37.0) \n",
      " If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\n"
     ]
    }
   ],
   "source": [
    "# Run Independent Samples T-test when assumptions are not violated.\n",
    "results = stats.ttest_ind(df_A[\"I found the application intuitive to use.\"],\n",
    "                          df_B[\"I found the application intuitive to use.\"])\n",
    "\n",
    "# Print the results\n",
    "print(f\"The results are significant if the p-value is significant, which means smaller than 0.05\", \n",
    "results,\n",
    "\"\\n\", \n",
    "\"If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results are significant if the p-value is significant, which means => 0.05 TtestResult(statistic=-1.0472272162506624, pvalue=0.3017879146791575, df=37.0) \n",
      " If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\n"
     ]
    }
   ],
   "source": [
    "# Run Bootstrapped Independent Samples T-test when assumptions are violated\n",
    "rng = np.random.default_rng() # create random sampling\n",
    "\n",
    "results = stats.ttest_ind(df_A[\"I found the application intuitive to use.\"],\n",
    "                          df_B[\"I found the application intuitive to use.\"],\n",
    "                          random_state = rng)\n",
    "\n",
    "# Print the results\n",
    "print(f\"The results are significant if the p-value is significant, which means => 0.05\", \n",
    "results,\n",
    "\"\\n\", \n",
    "\"If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the p-value is above 0.05, then the data is normally distrubted ShapiroResult(statistic=0.8219270437525785, pvalue=0.005406418999061028) . If the data is not normally distributed then you will have to run the bootstrapped version.\n",
      "If the p-value is above 0.05, then the data is normally distrubted ShapiroResult(statistic=0.722876615752247, pvalue=2.8139761873641475e-05) . If the data is not normally distributed then you will have to run the bootstrapped version.\n",
      "If the p-value is above 0.05, then the groups have equal variances LeveneResult(statistic=0.022678144886886763, pvalue=0.8811150663763754) . If the variance aren't equal then you will have to run the bootstrapped version.\n"
     ]
    }
   ],
   "source": [
    "# Run the shaprio-wilk statistical test for each question to check whether the data is normally distributed\n",
    "normal_a = stats.shapiro(df_A[\"I\\xa0thought the application was useful.\"])\n",
    "normal_b = stats.shapiro(df_B[\"I thought the application was useful.\"])\n",
    "\n",
    "# Check whether the variance of both samples is equal\n",
    "homogeneity = stats.levene(df_A[\"I\\xa0thought the application was useful.\"],\n",
    "                           df_B[\"I thought the application was useful.\"])\n",
    "\n",
    "print(f\"If the p-value is above 0.05, then the data is normally distrubted\", normal_a, \". If the data is not normally distributed then you will have to run the bootstrapped version.\")\n",
    "print(f\"If the p-value is above 0.05, then the data is normally distrubted\", normal_b, \". If the data is not normally distributed then you will have to run the bootstrapped version.\")\n",
    "print(f\"If the p-value is above 0.05, then the groups have equal variances\", homogeneity, \". If the variance aren't equal then you will have to run the bootstrapped version.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results are significant if the p-value is significant, which means smaller than 0.05 TtestResult(statistic=-0.7821579499548239, pvalue=0.43909882379629805, df=37.0) \n",
      " If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\n"
     ]
    }
   ],
   "source": [
    "# Run Independent Samples T-test when assumptions are not violated.\n",
    "results = stats.ttest_ind(df_A[\"I\\xa0thought the application was useful.\"],\n",
    "                          df_B[\"I thought the application was useful.\"])\n",
    "\n",
    "# Print the results\n",
    "print(f\"The results are significant if the p-value is significant, which means smaller than 0.05\", \n",
    "results,\n",
    "\"\\n\", \n",
    "\"If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results are significant if the p-value is significant, which means => 0.05 TtestResult(statistic=-0.7821579499548239, pvalue=0.43909882379629805, df=37.0) \n",
      " If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\n"
     ]
    }
   ],
   "source": [
    "# Run Bootstrapped Independent Samples T-test when assumptions are violated\n",
    "rng = np.random.default_rng() # create random sampling\n",
    "\n",
    "results = stats.ttest_ind(df_A[\"I\\xa0thought the application was useful.\"],\n",
    "                          df_B[\"I thought the application was useful.\"],\n",
    "                          random_state = rng)\n",
    "\n",
    "# Print the results\n",
    "print(f\"The results are significant if the p-value is significant, which means => 0.05\", \n",
    "results,\n",
    "\"\\n\", \n",
    "\"If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the p-value is above 0.05, then the data is normally distrubted ShapiroResult(statistic=0.8470651452461548, pvalue=0.012338410326657763) . If the data is not normally distributed then you will have to run the bootstrapped version.\n",
      "If the p-value is above 0.05, then the data is normally distrubted ShapiroResult(statistic=0.722876615752247, pvalue=2.8139761873641475e-05) . If the data is not normally distributed then you will have to run the bootstrapped version.\n",
      "If the p-value is above 0.05, then the groups have equal variances LeveneResult(statistic=0.0004943814219478673, pvalue=0.9823802266701495) . If the variance aren't equal then you will have to run the bootstrapped version.\n"
     ]
    }
   ],
   "source": [
    "# Run the shaprio-wilk statistical test for each question to check whether the data is normally distributed\n",
    "normal_a = stats.shapiro(df_A[\"I enjoyed using the application.\"])\n",
    "normal_b = stats.shapiro(df_B[\"I enjoyed using the application.\"])\n",
    "\n",
    "# Check whether the variance of both samples is equal\n",
    "homogeneity = stats.levene(df_A[\"I enjoyed using the application.\"],\n",
    "                           df_B[\"I enjoyed using the application.\"])\n",
    "\n",
    "print(f\"If the p-value is above 0.05, then the data is normally distrubted\", normal_a, \". If the data is not normally distributed then you will have to run the bootstrapped version.\")\n",
    "print(f\"If the p-value is above 0.05, then the data is normally distrubted\", normal_b, \". If the data is not normally distributed then you will have to run the bootstrapped version.\")\n",
    "print(f\"If the p-value is above 0.05, then the groups have equal variances\", homogeneity, \". If the variance aren't equal then you will have to run the bootstrapped version.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results are significant if the p-value is significant, which means smaller than 0.05 TtestResult(statistic=-1.3262617564193029, pvalue=0.19288199186454708, df=37.0) \n",
      " If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\n"
     ]
    }
   ],
   "source": [
    "# Run Independent Samples T-test when assumptions are not violated.\n",
    "results = stats.ttest_ind(df_A[\"I enjoyed using the application.\"],\n",
    "                          df_B[\"I enjoyed using the application.\"])\n",
    "\n",
    "# Print the results\n",
    "print(f\"The results are significant if the p-value is significant, which means smaller than 0.05\", \n",
    "results,\n",
    "\"\\n\", \n",
    "\"If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results are significant if the p-value is significant, which means => 0.05 TtestResult(statistic=-1.3262617564193029, pvalue=0.19288199186454708, df=37.0) \n",
      " If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\n"
     ]
    }
   ],
   "source": [
    "# Run Bootstrapped Independent Samples T-test when assumptions are violated\n",
    "rng = np.random.default_rng() # create random sampling\n",
    "\n",
    "results = stats.ttest_ind(df_A[\"I enjoyed using the application.\"],\n",
    "                          df_B[\"I enjoyed using the application.\"],\n",
    "                          random_state = rng)\n",
    "\n",
    "# Print the results\n",
    "print(f\"The results are significant if the p-value is significant, which means => 0.05\", \n",
    "results,\n",
    "\"\\n\", \n",
    "\"If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the p-value is above 0.05, then the data is normally distrubted ShapiroResult(statistic=0.8151405846534079, pvalue=0.004358231208181672) . If the data is not normally distributed then you will have to run the bootstrapped version.\n",
      "If the p-value is above 0.05, then the data is normally distrubted ShapiroResult(statistic=0.6977170438686422, pvalue=1.3077230328684643e-05) . If the data is not normally distributed then you will have to run the bootstrapped version.\n",
      "If the p-value is above 0.05, then the groups have equal variances LeveneResult(statistic=0.08473443701747307, pvalue=0.7726067242135081) . If the variance aren't equal then you will have to run the bootstrapped version.\n"
     ]
    }
   ],
   "source": [
    "# Run the shaprio-wilk statistical test for each question to check whether the data is normally distributed\n",
    "normal_a = stats.shapiro(df_A[\"The app flow was self explanatory\\xa0\"])\n",
    "normal_b = stats.shapiro(df_B[\"The app flow was self explanatory \"])\n",
    "\n",
    "# Check whether the variance of both samples is equal\n",
    "homogeneity = stats.levene(df_A[\"The app flow was self explanatory\\xa0\"],\n",
    "                           df_B[\"The app flow was self explanatory \"])\n",
    "\n",
    "print(f\"If the p-value is above 0.05, then the data is normally distrubted\", normal_a, \". If the data is not normally distributed then you will have to run the bootstrapped version.\")\n",
    "print(f\"If the p-value is above 0.05, then the data is normally distrubted\", normal_b, \". If the data is not normally distributed then you will have to run the bootstrapped version.\")\n",
    "print(f\"If the p-value is above 0.05, then the groups have equal variances\", homogeneity, \". If the variance aren't equal then you will have to run the bootstrapped version.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results are significant if the p-value is significant, which means smaller than 0.05 TtestResult(statistic=-1.7576929406677801, pvalue=0.08707134844947921, df=37.0) \n",
      " If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\n"
     ]
    }
   ],
   "source": [
    "# Run Independent Samples T-test when assumptions are not violated.\n",
    "results = stats.ttest_ind(df_A[\"The app flow was self explanatory\\xa0\"],\n",
    "                          df_B[\"The app flow was self explanatory \"])\n",
    "\n",
    "# Print the results\n",
    "print(f\"The results are significant if the p-value is significant, which means smaller than 0.05\", \n",
    "results,\n",
    "\"\\n\", \n",
    "\"If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results are significant if the p-value is significant, which means => 0.05 TtestResult(statistic=-1.7576929406677801, pvalue=0.08707134844947921, df=37.0) \n",
      " If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\n"
     ]
    }
   ],
   "source": [
    "# Run Bootstrapped Independent Samples T-test when assumptions are violated\n",
    "rng = np.random.default_rng() # create random sampling\n",
    "\n",
    "results = stats.ttest_ind(df_A[\"The app flow was self explanatory\\xa0\"],\n",
    "                          df_B[\"The app flow was self explanatory \"],\n",
    "                          random_state = rng)\n",
    "\n",
    "# Print the results\n",
    "print(f\"The results are significant if the p-value is significant, which means => 0.05\", \n",
    "results,\n",
    "\"\\n\", \n",
    "\"If the results are significant, that means that the version are different enough to exclude chance for being the driver. So if you version has a higher/lower average score and is statistically significant, then it works better/worse. If the results are not significant then the changes don't have a real measureable effect so maybe it's no better or maybe the questions don't really measure the effect and you should consider rephrase or removing them.There's more to it but that for inferential statistics in year 2.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "119c259948d333b2ddf4ba2ffb3d68be5171f28660b26be40acacf7136fda808"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
