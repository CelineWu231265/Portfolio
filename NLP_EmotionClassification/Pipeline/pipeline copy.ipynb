{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1 - Extract and translate sentences from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import assemblyai as aai\n",
    "import os\n",
    "from pytube import YouTube\n",
    "\n",
    "aai.settings.api_key = '4ba97f247dd44f86b2c51a29f14caa26'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aai.settings.api_key = ASSEMBLYAI_API_KEY\n",
    "\n",
    "# Path to your audio file\n",
    "audio_file = \"../Data/La isla de las tentaciones Temporada 7 capitulo 7.mp3\"\n",
    "\n",
    "# Configure transcription with Spanish language\n",
    "config = aai.TranscriptionConfig(language_code=\"es\")\n",
    "transcriber = aai.Transcriber(config=config)\n",
    "\n",
    "# Transcribe the audio\n",
    "transcript = transcriber.transcribe(audio_file)\n",
    "\n",
    "# Check for errors\n",
    "if transcript.status == aai.TranscriptStatus.error:\n",
    "    print(f\"Transcription failed: {transcript.error}\")\n",
    "    exit(1)\n",
    "\n",
    "# Create lists to store data\n",
    "sentences = []\n",
    "start_times = []\n",
    "end_times = []\n",
    "\n",
    "# Extract sentences with their timestamps\n",
    "sentence_objects = transcript.get_sentences()\n",
    "for sentence in sentence_objects:\n",
    "    sentences.append(sentence.text)\n",
    "    start_times.append(sentence.start)  # Start time in milliseconds\n",
    "    end_times.append(sentence.end)      # End time in milliseconds\n",
    "\n",
    "# Convert to DataFrame for easier manipulation\n",
    "transcript_df = pd.DataFrame({\n",
    "    'sentence': sentences,\n",
    "    'start_time_ms': start_times,\n",
    "    'end_time_ms': end_times\n",
    "})\n",
    "\n",
    "# Convert milliseconds to a more readable format\n",
    "transcript_df['start_time'] = transcript_df['start_time_ms'].apply(\n",
    "    lambda ms: f\"{int(ms/60000):02d}:{int((ms%60000)/1000):02d}.{int(ms%1000):03d}\"\n",
    ")\n",
    "transcript_df['end_time'] = transcript_df['end_time_ms'].apply(\n",
    "    lambda ms: f\"{int(ms/60000):02d}:{int((ms%60000)/1000):02d}.{int(ms%1000):03d}\"\n",
    ")\n",
    "\n",
    "# Format the DataFrame\n",
    "transcript_df = transcript_df[['start_time', 'end_time', 'sentence']]\n",
    "\n",
    "# Rename the columns as per template\n",
    "transcript_df.columns = ['Start Time', 'End Time', 'Sentence']\n",
    "\n",
    "# Display the first few rows to check\n",
    "print(transcript_df.head())\n",
    "\n",
    "# Save to CSV for the next step in your pipeline\n",
    "output_path = \"../task_11\"\n",
    "#transcript_df.to_csv(output_path, index=False)\n",
    "\n",
    "#print(f\"Extracted {len(sentences)} sentences with timestamps and saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Translate the sentences to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Buas/Desktop/Data Start/GitHub/Ano 2/2024-25c-fai2-adsai-DeuzaVarela235065/keras_env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/Buas/Desktop/Data Start/GitHub/Ano 2/2024-25c-fai2-adsai-DeuzaVarela235065/keras_env/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at /Users/Buas/Desktop/Data Start/GitHub/Ano 2/2024-25c-fai2-adsai-DeuzaVarela235065/task_11/translation_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
    "\n",
    "# === Load model and tokenizer ===\n",
    "model_path = \"/Users/Buas/Desktop/Data Start/GitHub/Ano 2/2024-25c-fai2-adsai-DeuzaVarela235065/task_11/translation_model\"  # <-- Replace with your local model path\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Excel file ===\n",
    "input_path = \"./extracted_sentences.csv\"   \n",
    "df = pd.read_csv(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load Excel file ===\n",
    "input_path = \"./extracted_sentences.csv\"   \n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# === Translate a column of sentences ===\n",
    "def translate(text):\n",
    "\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"tf\", padding=True, truncation=True, max_length=256)\n",
    "    # Generate translation\n",
    "    outputs = model.generate(inputs, max_length=256)\n",
    "    # Decode the output\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Apply translation to the column (replace 'text' with your actual column name)\n",
    "df['translated_sentence'] = df['Sentence'].apply(translate)\n",
    "\n",
    "# === Save to new Excel file ===\n",
    "output_path = \"./extracted_sentences.csv\"   # <-- Replace with desired output path\n",
    "df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
