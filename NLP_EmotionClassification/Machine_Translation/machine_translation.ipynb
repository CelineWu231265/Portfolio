{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6 Machine Translation  \n",
    "\n",
    "### This was following the tutorial provided to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "from transformers import AdamWeightDecay\n",
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
    "from ftfy import fix_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Helsinki-NLP/opus-mt-es-en\" #fetching pretrained translation model from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emilp\\anaconda3\\envs\\my_tf_env\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\emilp\\.cache\\huggingface\\hub\\datasets--Helsinki-NLP--opus_books. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 93470/93470 [00:00<00:00, 395030.79 examples/s]\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = load_dataset(\"Helsinki-NLP/opus_books\", \"en-es\") #getting english to spanish dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = raw_datasets['train'].train_test_split(test_size=0.01)  # split 10% for test\n",
    "\n",
    "validation_test_split = train_test_split['train'].train_test_split(test_size=0.01)  # 10% of 90% => 9% of original\n",
    "\n",
    "# irganizing the splits into a new DatasetDict\n",
    "raw_datasets = DatasetDict({\n",
    "    'train': validation_test_split['train'],\n",
    "    'validation': validation_test_split['test'],\n",
    "    'test': train_test_split['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emilp\\anaconda3\\envs\\my_tf_env\\lib\\site-packages\\huggingface_hub\\file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\emilp\\anaconda3\\envs\\my_tf_env\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\emilp\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-es-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\emilp\\anaconda3\\envs\\my_tf_env\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128 #max 128 tokens\n",
    "\n",
    "source_lang = \"es\"\n",
    "target_lang = \"en\" #setting it up so that it translates from spanish to english\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]] # it gathers spansish sentences as input and english sentences as targets \n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True) # tokenizes the sentences\n",
    "\n",
    "    # setting up the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emilp\\anaconda3\\envs\\my_tf_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3635: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[711, 11673, 15, 74, 17496, 152, 4, 28, 650, 8, 526, 2, 11, 194, 3296, 12, 155, 10594, 26, 165, 565, 6, 8, 17163, 15, 74, 21972, 326, 2, 461, 2, 4, 25, 9, 2406, 33738, 2, 74, 4, 9, 27503, 28, 9563, 28851, 3, 0], [32, 14718, 1486, 15, 8924, 37066, 74, 4602, 4712, 15, 37, 15, 74, 39462, 6, 1974, 51, 37, 6, 49535, 660, 575, 1508, 2, 1760, 668, 37, 236, 6, 6202, 19, 1508, 15, 155, 3398, 2847, 484, 2, 488, 43, 212, 15, 12, 575, 10257, 187, 3135, 17368, 660, 14, 163, 4, 8275, 12, 20490, 3, 42368, 15, 1533, 1711, 212, 37, 26, 9558, 575, 1508, 3, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[33, 9567, 174, 95, 2201, 27, 14424, 16, 125, 1883, 51, 23, 2, 81, 73, 1540, 2, 504, 5477, 2, 95, 17752, 13795, 10, 7605, 3, 0], [1036, 50760, 2, 5218, 5, 17394, 1735, 7, 5, 21725, 35, 119, 32016, 2, 55991, 2, 52, 18879, 9920, 3618, 13, 44, 6856, 3585, 2411, 235, 20842, 9, 93, 125, 1246, 51, 6542, 48, 64, 147, 21016, 250, 2, 23, 48, 31, 64, 13, 14193, 23, 5, 7050, 125, 449, 82, 850, 40, 2, 3469, 31, 48, 209, 13, 6824, 100, 3711, 81, 83, 44472, 35, 147, 954, 8, 439, 51, 2743, 27, 105, 138, 12450, 147, 255, 130, 64, 26909, 11183, 147, 9326, 250, 395, 0]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(raw_datasets[\"train\"][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 92536/92536 [00:41<00:00, 2236.44 examples/s]\n",
      "Map: 100%|██████████| 466/466 [00:00<00:00, 2519.69 examples/s]\n",
      "Map: 100%|██████████| 468/468 [00:00<00:00, 1934.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True) #converts es to en translation pairs into tokenized tensors that the model can learn from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-es-en.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint) # loads the appropriate architecure automatically "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.01\n",
    "num_train_epochs = 1 # all parameters subject to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\") #formats and pads a list of examples (the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\", pad_to_multiple_of=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = model.prepare_tf_dataset( #creates and prepares the dataset for training\n",
    "    tokenized_datasets[\"test\"], #it uses the test because the dataset is so large it would take far too long to train but it should be \"train\"\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "validation_dataset = model.prepare_tf_dataset( #this one is user for validatioin in training\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "generation_dataset = model.prepare_tf_dataset( #uses validation set but for inference\n",
    "    tokenized_datasets[\"validation\"],\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=generation_data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 341s 10s/step - loss: 2.5088 - val_loss: 2.3708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d4bfec18e0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, validation_data=validation_dataset, epochs=1) #training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('my_local_model\\\\tokenizer_config.json',\n",
       " 'my_local_model\\\\special_tokens_map.json',\n",
       " 'my_local_model\\\\vocab.json',\n",
       " 'my_local_model\\\\source.spm',\n",
       " 'my_local_model\\\\target.spm',\n",
       " 'my_local_model\\\\added_tokens.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"my_local_model\")         # saves config + weights\n",
    "tokenizer.save_pretrained(\"my_local_model\")     # saves tokenizer files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at my_local_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n",
      "c:\\Users\\emilp\\anaconda3\\envs\\my_tf_env\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"my_local_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"my_local_model\") #loading model and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to translate spanish sentences from an excel file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emilp\\anaconda3\\envs\\my_tf_env\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at my_local_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"my_local_model\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "input_path = r\"C:\\Users\\emilp\\Documents\\GitHub\\2024-25c-fai2-adsai-EmilFox231007\\datalab_tasks\\Task_11\\extracted_sentences.csv\"    # <-- Replace with your Excel file path\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "def translate(text):\n",
    "\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"tf\", padding=True, truncation=True, max_length=256)\n",
    "    # Generate translation\n",
    "    outputs = model.generate(inputs, max_length=256)\n",
    "    # Decode the output\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Apply translation to the column \n",
    "df['translated_sentence'] = df['Sentence'].apply(translate)\n",
    "\n",
    "# save to excel file\n",
    "output_path = r\"C:\\Users\\emilp\\Documents\\GitHub\\2024-25c-fai2-adsai-EmilFox231007\\datalab_tasks\\Task_11\\extracted_sentences.csv\"   # <-- Replace with desired output path\n",
    "df.to_csv(output_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
