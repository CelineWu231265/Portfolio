{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/celinewu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model \n",
    "nlp_es = spacy.load('es_core_news_sm')\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel(\"/Users/celinewu/Documents/GitHub/2024-25c-fai2-adsai-group-group16/Data/transcribed_data_whisper.xlsx\")\n",
    "\n",
    "# Translate sentences to English using spaCy\n",
    "def translate_to_english(sentence):\n",
    "    doc = nlp_es(sentence)\n",
    "    return ' '.join([token.text for token in doc]) \n",
    "\n",
    "df['Sentence_English'] = df['Sentence'].apply(translate_to_english)\n",
    "\n",
    "# POS Tagging\n",
    "def pos_tagging(sentence):\n",
    "    doc = nlp_en(sentence)\n",
    "    return ' '.join([token.pos_ for token in doc])\n",
    "df['POS_Tags'] = df['Sentence_English'].apply(pos_tagging)\n",
    "\n",
    "# TF-IDF Calculation\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['Sentence_English'])\n",
    "df['TF_IDF'] = list(tfidf_matrix.toarray())\n",
    "\n",
    "# Sentiment Analysis\n",
    "def sentiment_score(sentence):\n",
    "    return TextBlob(sentence).sentiment.polarity\n",
    "df['Sentiment_Score'] = df['Sentence_English'].apply(sentiment_score)\n",
    "\n",
    "# Pretrained Word Embeddings\n",
    "def get_word_embedding(sentence, word_vectors):\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    vectors = [word_vectors[word] for word in words if word in word_vectors]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(300)  \n",
    "\n",
    "# Load pretrained embeddings \n",
    "word_vectors = {}  \n",
    "df['Pretrained_Embeddings'] = df['Sentence_English'].apply(lambda x: get_word_embedding(x, word_vectors))\n",
    "\n",
    "# Custom Word Embedding Model\n",
    "sentences = [word_tokenize(sentence.lower()) for sentence in df['Sentence_English']]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=300, window=5, min_count=1, workers=4)\n",
    "\n",
    "def get_custom_embedding(sentence, model):\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(300)\n",
    "df['Custom_Embeddings'] = df['Sentence_English'].apply(lambda x: get_custom_embedding(x, word2vec_model))\n",
    "\n",
    "# Additional Feature: Sentence Length\n",
    "df['Sentence_Length'] = df['Sentence_English'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Save to file \n",
    "df.to_csv('NLP_features.tsv', sep='\\t', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation complete. File saved as 'translated_data.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Load the dataset \n",
    "df = pd.read_excel(\"/Users/celinewu/Documents/GitHub/2024-25c-fai2-adsai-group-group16/Data/transcribed_data_whisper.xlsx\")\n",
    "\n",
    "# Initialize GoogleTranslator\n",
    "translator = GoogleTranslator(source='es', target='en')\n",
    "\n",
    "# Function to translate Spanish text to English\n",
    "def translate_to_english(sentence):\n",
    "    if pd.isna(sentence) or not isinstance(sentence, str):\n",
    "        return \"\"  \n",
    "    try:\n",
    "        return translator.translate(sentence)\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error: {e}\")\n",
    "        return sentence \n",
    "\n",
    "# Apply translation\n",
    "df['Sentence_English'] = df['Sentence'].apply(translate_to_english)\n",
    "\n",
    "# Save the translated dataset\n",
    "df.to_excel(\"translated_data_whisper.xlsx\", index=False)\n",
    "\n",
    "print(\"Translation complete. File saved as 'translated_data.xlsx'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2B",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
