{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of the Notebook\n",
    "This notebook takes a dataset of Spanish sentences and applies Natural Language Processing (NLP) to analyze and transform the text into useful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"../Documents/Preliminary feedback task 4_team16.pdf\" target=\"_blank\">Open PDF</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML('<a href=\"../Documents/Preliminary feedback task 4_team16.pdf\" target=\"_blank\">Open PDF</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Libraries**\n",
    "\n",
    "- **pandas** → Handles data in tables\n",
    "- **spaCy** → Identifies parts of speech (nouns, verbs, etc.)\n",
    "- **NumPy** → Performs mathematical operations\n",
    "- **NLTK** → Breaks sentences into words\n",
    "- **GoogleTranslator** → Translates Spanish text to English\n",
    "- **TfidfVectorizer** → Converts words into numerical importance scores\n",
    "- **TextBlob** → Analyzes sentiment (positive or negative)\n",
    "- **Word2Vec** → Converts words into numerical representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Feature Name**           | **Description**                                                    |\n",
    "|----------------------------|--------------------------------------------------------------------|\n",
    "| **Sentence_English**       | English translation of the Spanish sentence                        |\n",
    "| **POS_Tags**               | Part-of-speech tags for each word (e.g., nouns, verbs, adjectives) |\n",
    "| **TF-IDF**                 | Word importance scores based on Term Frequency–Inverse Document Frequency |\n",
    "| **Sentiment_Score**        | Polarity score indicating if the sentence is positive or negative  |\n",
    "| **Pretrained_Embeddings**  | Sentence-level vector using Google’s pretrained Word2Vec model     |\n",
    "| **Custom_Embeddings**      | Sentence-level vector using a Word2Vec model trained on the dataset |\n",
    "| **Sentence_Length**        | Number of tokens in the cleaned sentence  \n",
    "\n",
    "\n",
    "#### Methodology with Code Snippets and Descriptions\n",
    "\n",
    "| **Step**                        | **Why** (Purpose of the Step)                                                                                                      | **Code Snippet**                                                                                                                   |\n",
    "|---------------------------------|-------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Data Loading**                | To begin processing, we need to load the dataset and ensure there are no missing values that could break downstream operations.    | <pre>df = pd.read_csv('path/to/augmented_dataset.csv')<br>df['text'] = df['text'].fillna('')</pre>                                |\n",
    "| **POS Tagging**                 | Part-of-speech tags reveal sentence structure and grammatical roles, which can influence how emotions are expressed.               | <pre>def pos_tagging(text):<br>    doc = nlp_en(text)<br>    return [token.pos_ for token in doc]<br>df['POS_Tags'] = df['text'].apply(pos_tagging)</pre> |\n",
    "| **Text Cleaning**              | To remove noise and standardise the input, ensuring the models and features are based on relevant, clean tokens.                   | <pre>def clean_text(text):<br>    ...  # your full cleaning logic<br>df['Clean_Sentence'] = df['text'].apply(clean_text)</pre>     |\n",
    "| **TF-IDF Calculation**          | Highlights which words are most important in each sentence, helping models focus on emotionally meaningful terms.                  | <pre>vectorizer = TfidfVectorizer()<br>tfidf_matrix = vectorizer.fit_transform(df['Clean_Sentence'])<br>df['TF_IDF'] = list(tfidf_matrix.toarray())</pre> |\n",
    "| **Sentiment Analysis**          | Captures the emotional tone (positive, negative, or neutral) of each sentence as a numerical feature.                              | <pre>def sentiment_score(text):<br>    return TextBlob(text).sentiment.polarity<br>df['Sentiment_Score'] = df['text'].apply(sentiment_score)</pre> |\n",
    "| **Pretrained Word Embeddings**  | Uses external linguistic knowledge (Google News corpus) to convert words into meaningful numeric representations.                   | <pre>word_vectors = api.load(\"word2vec-google-news-300\")<br>def get_embedding(text):<br>    words = word_tokenize(text.lower())<br>    vectors = [word_vectors[word] for word in words if word in word_vectors]<br>    return np.mean(vectors, axis=0) if vectors else np.zeros(300)<br>df['Pretrained_Embeddings'] = df['text'].apply(get_embedding)</pre> |\n",
    "| **Custom Word2Vec Embeddings**  | Trains embeddings on your specific dataset to capture context-specific emotional expressions not covered in pretrained models.      | <pre>corpus = df['Clean_Sentence'].apply(word_tokenize).tolist()<br>custom_model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4)<br>def get_custom_embedding(text):<br>    tokens = word_tokenize(text)<br>    vectors = [custom_model.wv[token] for token in tokens if token in custom_model.wv]<br>    return np.mean(vectors, axis=0) if vectors else np.zeros(100)<br>df['Custom_Embeddings'] = df['Clean_Sentence'].apply(get_custom_embedding)</pre> |\n",
    "| **Sentence Length Calculation** | Emotionally rich sentences tend to vary in length; token count is a simple but effective feature capturing that variation.          | <pre>df['Sentence_Length'] = df['Clean_Sentence'].apply(lambda x: len(word_tokenize(x)))</pre>                                     |\n",
    "| **Output Saving**               | Saves all engineered features into a file for model training, sharing, or further analysis.                                        | <pre>df.to_excel('FINAL_DATASET.xlsx', index=False)</pre>                                                                          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Buas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/Buas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /Users/Buas/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "# Data manipulation and numerical operations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Natural Language Processing libraries\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import TextBlob\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Machine Learning and Feature Extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Word Embeddings\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Load spaCy models for Spanish and English\n",
    "nlp_es = spacy.load('es_core_news_sm')\n",
    "nlp_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize stopwords\n",
    "stop_words = set(stopwords.words('english'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loaded the data from an Excel file into a Pandas DataFrame\n",
    "- Created a translator object (translator = Translator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>main_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>there is absolutely no question personality ef...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i cannot find year stats anyhow i needed the e...</td>\n",
       "      <td>disgust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hated her and that was honestly a more offensi...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pikachu shocked i hope we give him a fair chan...</td>\n",
       "      <td>happiness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worst bit is that it keeps snowballing and gro...</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text main_category\n",
       "0  there is absolutely no question personality ef...     happiness\n",
       "1  i cannot find year stats anyhow i needed the e...       disgust\n",
       "2  hated her and that was honestly a more offensi...         anger\n",
       "3  pikachu shocked i hope we give him a fair chan...     happiness\n",
       "4  worst bit is that it keeps snowballing and gro...         anger"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df =  pd.read_csv('../task_4/augmented_dataset_reduced.csv') \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>main_category</th>\n",
       "      <th>POS_Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>there is absolutely no question personality ef...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>[PRON, VERB, ADV, DET, NOUN, NOUN, NOUN, DET, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i cannot find year stats anyhow i needed the e...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>[PRON, AUX, PART, VERB, NOUN, NOUN, ADV, PRON,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hated her and that was honestly a more offensi...</td>\n",
       "      <td>anger</td>\n",
       "      <td>[VERB, PRON, CCONJ, PRON, AUX, ADV, DET, ADV, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pikachu shocked i hope we give him a fair chan...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>[PROPN, VERB, PRON, VERB, PRON, VERB, PRON, DE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worst bit is that it keeps snowballing and gro...</td>\n",
       "      <td>anger</td>\n",
       "      <td>[ADJ, NOUN, AUX, SCONJ, PRON, VERB, VERB, CCON...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text main_category  \\\n",
       "0  there is absolutely no question personality ef...     happiness   \n",
       "1  i cannot find year stats anyhow i needed the e...       disgust   \n",
       "2  hated her and that was honestly a more offensi...         anger   \n",
       "3  pikachu shocked i hope we give him a fair chan...     happiness   \n",
       "4  worst bit is that it keeps snowballing and gro...         anger   \n",
       "\n",
       "                                            POS_Tags  \n",
       "0  [PRON, VERB, ADV, DET, NOUN, NOUN, NOUN, DET, ...  \n",
       "1  [PRON, AUX, PART, VERB, NOUN, NOUN, ADV, PRON,...  \n",
       "2  [VERB, PRON, CCONJ, PRON, AUX, ADV, DET, ADV, ...  \n",
       "3  [PROPN, VERB, PRON, VERB, PRON, VERB, PRON, DE...  \n",
       "4  [ADJ, NOUN, AUX, SCONJ, PRON, VERB, VERB, CCON...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS Tagging\n",
    "# Changed POS tag representation from a concatenated string to a list.\n",
    "# Previously, the POS tags were joined into a single string (e.g., \"NOUN VERB ADJ\"),\n",
    "# which lost the sequential structure and individual token information.\n",
    "# By storing the POS tags as a list, we preserve their order and details,\n",
    "# enabling more effective downstream processing (e.g., one-hot encoding or embedding)\n",
    "# and potentially enhancing model performance.\n",
    "def pos_tagging_list(text):\n",
    "    doc = nlp_en(text)\n",
    "    return [token.pos_ for token in doc]\n",
    "\n",
    "# run funcion \n",
    "df['POS_Tags'] = df['text'].apply(pos_tagging_list)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>main_category</th>\n",
       "      <th>POS_Tags</th>\n",
       "      <th>Clean_Sentence</th>\n",
       "      <th>TF_IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>there is absolutely no question personality ef...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>[PRON, VERB, ADV, DET, NOUN, NOUN, NOUN, DET, ...</td>\n",
       "      <td>absolutely question personality effects outcom...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i cannot find year stats anyhow i needed the e...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>[PRON, AUX, PART, VERB, NOUN, NOUN, ADV, PRON,...</td>\n",
       "      <td>find year stats anyhow needed excel files list...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hated her and that was honestly a more offensi...</td>\n",
       "      <td>anger</td>\n",
       "      <td>[VERB, PRON, CCONJ, PRON, AUX, ADV, DET, ADV, ...</td>\n",
       "      <td>hated honestly offensive reaction everyone lau...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pikachu shocked i hope we give him a fair chan...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>[PROPN, VERB, PRON, VERB, PRON, VERB, PRON, DE...</td>\n",
       "      <td>pikachu shocked hope give fair chance succeeds</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worst bit is that it keeps snowballing and gro...</td>\n",
       "      <td>anger</td>\n",
       "      <td>[ADJ, NOUN, AUX, SCONJ, PRON, VERB, VERB, CCON...</td>\n",
       "      <td>worst bit keeps snowballing growing suddenly m...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text main_category  \\\n",
       "0  there is absolutely no question personality ef...     happiness   \n",
       "1  i cannot find year stats anyhow i needed the e...       disgust   \n",
       "2  hated her and that was honestly a more offensi...         anger   \n",
       "3  pikachu shocked i hope we give him a fair chan...     happiness   \n",
       "4  worst bit is that it keeps snowballing and gro...         anger   \n",
       "\n",
       "                                            POS_Tags  \\\n",
       "0  [PRON, VERB, ADV, DET, NOUN, NOUN, NOUN, DET, ...   \n",
       "1  [PRON, AUX, PART, VERB, NOUN, NOUN, ADV, PRON,...   \n",
       "2  [VERB, PRON, CCONJ, PRON, AUX, ADV, DET, ADV, ...   \n",
       "3  [PROPN, VERB, PRON, VERB, PRON, VERB, PRON, DE...   \n",
       "4  [ADJ, NOUN, AUX, SCONJ, PRON, VERB, VERB, CCON...   \n",
       "\n",
       "                                      Clean_Sentence  \\\n",
       "0  absolutely question personality effects outcom...   \n",
       "1  find year stats anyhow needed excel files list...   \n",
       "2  hated honestly offensive reaction everyone lau...   \n",
       "3     pikachu shocked hope give fair chance succeeds   \n",
       "4  worst bit keeps snowballing growing suddenly m...   \n",
       "\n",
       "                                              TF_IDF  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Pre-processing Improvement\n",
    "#  - Converting text to lowercase\n",
    "#  - Removing digits and punctuation\n",
    "#  - Tokenizing the text\n",
    "#  - Removing stopwords\n",
    "#  - remove links and URLs\n",
    "#  - Removing extra whitespace\n",
    "#  - Removing mentions and hashtags\n",
    "#  - Removing non-alphabetic characters\n",
    "#  - Removing newlines\n",
    "#  - Removing punctuation\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE) # MULTILINE for regex pattern on a per-line basis \n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)  # Remove mentions\n",
    "    text = re.sub(r'#', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[\\r\\n]+', ' ', text)  # Remove newlines\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenize the text\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply the cleaning function to the sentences\n",
    "df['Clean_Sentence'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Now apply TF-IDF on the cleaned text:\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['Clean_Sentence'])\n",
    "df['TF_IDF'] = list(tfidf_matrix.toarray())\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>main_category</th>\n",
       "      <th>POS_Tags</th>\n",
       "      <th>Clean_Sentence</th>\n",
       "      <th>TF_IDF</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>there is absolutely no question personality ef...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>[PRON, VERB, ADV, DET, NOUN, NOUN, NOUN, DET, ...</td>\n",
       "      <td>absolutely question personality effects outcom...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>-0.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i cannot find year stats anyhow i needed the e...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>[PRON, AUX, PART, VERB, NOUN, NOUN, ADV, PRON,...</td>\n",
       "      <td>find year stats anyhow needed excel files list...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hated her and that was honestly a more offensi...</td>\n",
       "      <td>anger</td>\n",
       "      <td>[VERB, PRON, CCONJ, PRON, AUX, ADV, DET, ADV, ...</td>\n",
       "      <td>hated honestly offensive reaction everyone lau...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>-0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pikachu shocked i hope we give him a fair chan...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>[PROPN, VERB, PRON, VERB, PRON, VERB, PRON, DE...</td>\n",
       "      <td>pikachu shocked hope give fair chance succeeds</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.233333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worst bit is that it keeps snowballing and gro...</td>\n",
       "      <td>anger</td>\n",
       "      <td>[ADJ, NOUN, AUX, SCONJ, PRON, VERB, VERB, CCON...</td>\n",
       "      <td>worst bit keeps snowballing growing suddenly m...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>-0.268750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text main_category  \\\n",
       "0  there is absolutely no question personality ef...     happiness   \n",
       "1  i cannot find year stats anyhow i needed the e...       disgust   \n",
       "2  hated her and that was honestly a more offensi...         anger   \n",
       "3  pikachu shocked i hope we give him a fair chan...     happiness   \n",
       "4  worst bit is that it keeps snowballing and gro...         anger   \n",
       "\n",
       "                                            POS_Tags  \\\n",
       "0  [PRON, VERB, ADV, DET, NOUN, NOUN, NOUN, DET, ...   \n",
       "1  [PRON, AUX, PART, VERB, NOUN, NOUN, ADV, PRON,...   \n",
       "2  [VERB, PRON, CCONJ, PRON, AUX, ADV, DET, ADV, ...   \n",
       "3  [PROPN, VERB, PRON, VERB, PRON, VERB, PRON, DE...   \n",
       "4  [ADJ, NOUN, AUX, SCONJ, PRON, VERB, VERB, CCON...   \n",
       "\n",
       "                                      Clean_Sentence  \\\n",
       "0  absolutely question personality effects outcom...   \n",
       "1  find year stats anyhow needed excel files list...   \n",
       "2  hated honestly offensive reaction everyone lau...   \n",
       "3     pikachu shocked hope give fair chance succeeds   \n",
       "4  worst bit keeps snowballing growing suddenly m...   \n",
       "\n",
       "                                              TF_IDF  Sentiment_Score  \n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        -0.050000  \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         0.000000  \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        -0.200000  \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         0.233333  \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        -0.268750  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment Analysis\n",
    "def sentiment_score(sentence):\n",
    "    return TextBlob(sentence).sentiment.polarity # Polarity measures the overall tone of a text, indicating whether it is positive, negative, or neutral\n",
    "\n",
    "# sanity check\n",
    "df['Sentiment_Score'] = df['text'].apply(sentiment_score)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>main_category</th>\n",
       "      <th>POS_Tags</th>\n",
       "      <th>Clean_Sentence</th>\n",
       "      <th>TF_IDF</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "      <th>Pretrained_Embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>there is absolutely no question personality ef...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>[PRON, VERB, ADV, DET, NOUN, NOUN, NOUN, DET, ...</td>\n",
       "      <td>absolutely question personality effects outcom...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>[0.06916882, -0.004824684, 0.030941918, 0.0643...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i cannot find year stats anyhow i needed the e...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>[PRON, AUX, PART, VERB, NOUN, NOUN, ADV, PRON,...</td>\n",
       "      <td>find year stats anyhow needed excel files list...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.024938246, 0.053811464, 0.06385085, 0.08510...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hated her and that was honestly a more offensi...</td>\n",
       "      <td>anger</td>\n",
       "      <td>[VERB, PRON, CCONJ, PRON, AUX, ADV, DET, ADV, ...</td>\n",
       "      <td>hated honestly offensive reaction everyone lau...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>[0.061091498, -0.051133376, 0.0779149, 0.09205...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pikachu shocked i hope we give him a fair chan...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>[PROPN, VERB, PRON, VERB, PRON, VERB, PRON, DE...</td>\n",
       "      <td>pikachu shocked hope give fair chance succeeds</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>[0.057531737, 0.0421875, 0.100134276, 0.069775...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worst bit is that it keeps snowballing and gro...</td>\n",
       "      <td>anger</td>\n",
       "      <td>[ADJ, NOUN, AUX, SCONJ, PRON, VERB, VERB, CCON...</td>\n",
       "      <td>worst bit keeps snowballing growing suddenly m...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>-0.268750</td>\n",
       "      <td>[0.10927473, 0.010367076, 0.029471261, 0.10525...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text main_category  \\\n",
       "0  there is absolutely no question personality ef...     happiness   \n",
       "1  i cannot find year stats anyhow i needed the e...       disgust   \n",
       "2  hated her and that was honestly a more offensi...         anger   \n",
       "3  pikachu shocked i hope we give him a fair chan...     happiness   \n",
       "4  worst bit is that it keeps snowballing and gro...         anger   \n",
       "\n",
       "                                            POS_Tags  \\\n",
       "0  [PRON, VERB, ADV, DET, NOUN, NOUN, NOUN, DET, ...   \n",
       "1  [PRON, AUX, PART, VERB, NOUN, NOUN, ADV, PRON,...   \n",
       "2  [VERB, PRON, CCONJ, PRON, AUX, ADV, DET, ADV, ...   \n",
       "3  [PROPN, VERB, PRON, VERB, PRON, VERB, PRON, DE...   \n",
       "4  [ADJ, NOUN, AUX, SCONJ, PRON, VERB, VERB, CCON...   \n",
       "\n",
       "                                      Clean_Sentence  \\\n",
       "0  absolutely question personality effects outcom...   \n",
       "1  find year stats anyhow needed excel files list...   \n",
       "2  hated honestly offensive reaction everyone lau...   \n",
       "3     pikachu shocked hope give fair chance succeeds   \n",
       "4  worst bit keeps snowballing growing suddenly m...   \n",
       "\n",
       "                                              TF_IDF  Sentiment_Score  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        -0.050000   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         0.000000   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        -0.200000   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         0.233333   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        -0.268750   \n",
       "\n",
       "                               Pretrained_Embeddings  \n",
       "0  [0.06916882, -0.004824684, 0.030941918, 0.0643...  \n",
       "1  [0.024938246, 0.053811464, 0.06385085, 0.08510...  \n",
       "2  [0.061091498, -0.051133376, 0.0779149, 0.09205...  \n",
       "3  [0.057531737, 0.0421875, 0.100134276, 0.069775...  \n",
       "4  [0.10927473, 0.010367076, 0.029471261, 0.10525...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pretrained embeddings \n",
    "word_vectors = api.load(\"word2vec-google-news-300\")  \n",
    "\n",
    "def get_word_embedding(sentence, word_vectors):\n",
    "    words = word_tokenize(sentence.lower())\n",
    "    vectors = [word_vectors[word] for word in words if word in word_vectors]\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(300)\n",
    "\n",
    "# sanity check\n",
    "df['Pretrained_Embeddings'] = df['text'].apply(lambda x: get_word_embedding(x, word_vectors))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>main_category</th>\n",
       "      <th>POS_Tags</th>\n",
       "      <th>Clean_Sentence</th>\n",
       "      <th>TF_IDF</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "      <th>Pretrained_Embeddings</th>\n",
       "      <th>tokens</th>\n",
       "      <th>Custom_Embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>there is absolutely no question personality ef...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>[PRON, VERB, ADV, DET, NOUN, NOUN, NOUN, DET, ...</td>\n",
       "      <td>absolutely question personality effects outcom...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>[0.06916882, -0.004824684, 0.030941918, 0.0643...</td>\n",
       "      <td>[absolutely, question, personality, effects, o...</td>\n",
       "      <td>[-0.0019956154, 0.012318241, 0.0036612581, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i cannot find year stats anyhow i needed the e...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>[PRON, AUX, PART, VERB, NOUN, NOUN, ADV, PRON,...</td>\n",
       "      <td>find year stats anyhow needed excel files list...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[0.024938246, 0.053811464, 0.06385085, 0.08510...</td>\n",
       "      <td>[find, year, stats, anyhow, needed, excel, fil...</td>\n",
       "      <td>[-0.0045811804, 0.013830799, 0.0021979574, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hated her and that was honestly a more offensi...</td>\n",
       "      <td>anger</td>\n",
       "      <td>[VERB, PRON, CCONJ, PRON, AUX, ADV, DET, ADV, ...</td>\n",
       "      <td>hated honestly offensive reaction everyone lau...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>[0.061091498, -0.051133376, 0.0779149, 0.09205...</td>\n",
       "      <td>[hated, honestly, offensive, reaction, everyon...</td>\n",
       "      <td>[-0.008252272, 0.017440444, 0.0027486824, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pikachu shocked i hope we give him a fair chan...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>[PROPN, VERB, PRON, VERB, PRON, VERB, PRON, DE...</td>\n",
       "      <td>pikachu shocked hope give fair chance succeeds</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>[0.057531737, 0.0421875, 0.100134276, 0.069775...</td>\n",
       "      <td>[pikachu, shocked, hope, give, fair, chance, s...</td>\n",
       "      <td>[-0.0032568697, 0.012707858, 0.0024554152, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worst bit is that it keeps snowballing and gro...</td>\n",
       "      <td>anger</td>\n",
       "      <td>[ADJ, NOUN, AUX, SCONJ, PRON, VERB, VERB, CCON...</td>\n",
       "      <td>worst bit keeps snowballing growing suddenly m...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>-0.268750</td>\n",
       "      <td>[0.10927473, 0.010367076, 0.029471261, 0.10525...</td>\n",
       "      <td>[worst, bit, keeps, snowballing, growing, sudd...</td>\n",
       "      <td>[-0.007715346, 0.02062087, 0.0048620435, -0.00...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text main_category  \\\n",
       "0  there is absolutely no question personality ef...     happiness   \n",
       "1  i cannot find year stats anyhow i needed the e...       disgust   \n",
       "2  hated her and that was honestly a more offensi...         anger   \n",
       "3  pikachu shocked i hope we give him a fair chan...     happiness   \n",
       "4  worst bit is that it keeps snowballing and gro...         anger   \n",
       "\n",
       "                                            POS_Tags  \\\n",
       "0  [PRON, VERB, ADV, DET, NOUN, NOUN, NOUN, DET, ...   \n",
       "1  [PRON, AUX, PART, VERB, NOUN, NOUN, ADV, PRON,...   \n",
       "2  [VERB, PRON, CCONJ, PRON, AUX, ADV, DET, ADV, ...   \n",
       "3  [PROPN, VERB, PRON, VERB, PRON, VERB, PRON, DE...   \n",
       "4  [ADJ, NOUN, AUX, SCONJ, PRON, VERB, VERB, CCON...   \n",
       "\n",
       "                                      Clean_Sentence  \\\n",
       "0  absolutely question personality effects outcom...   \n",
       "1  find year stats anyhow needed excel files list...   \n",
       "2  hated honestly offensive reaction everyone lau...   \n",
       "3     pikachu shocked hope give fair chance succeeds   \n",
       "4  worst bit keeps snowballing growing suddenly m...   \n",
       "\n",
       "                                              TF_IDF  Sentiment_Score  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        -0.050000   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         0.000000   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        -0.200000   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...         0.233333   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...        -0.268750   \n",
       "\n",
       "                               Pretrained_Embeddings  \\\n",
       "0  [0.06916882, -0.004824684, 0.030941918, 0.0643...   \n",
       "1  [0.024938246, 0.053811464, 0.06385085, 0.08510...   \n",
       "2  [0.061091498, -0.051133376, 0.0779149, 0.09205...   \n",
       "3  [0.057531737, 0.0421875, 0.100134276, 0.069775...   \n",
       "4  [0.10927473, 0.010367076, 0.029471261, 0.10525...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [absolutely, question, personality, effects, o...   \n",
       "1  [find, year, stats, anyhow, needed, excel, fil...   \n",
       "2  [hated, honestly, offensive, reaction, everyon...   \n",
       "3  [pikachu, shocked, hope, give, fair, chance, s...   \n",
       "4  [worst, bit, keeps, snowballing, growing, sudd...   \n",
       "\n",
       "                                   Custom_Embeddings  \n",
       "0  [-0.0019956154, 0.012318241, 0.0036612581, 0.0...  \n",
       "1  [-0.0045811804, 0.013830799, 0.0021979574, -0....  \n",
       "2  [-0.008252272, 0.017440444, 0.0027486824, -0.0...  \n",
       "3  [-0.0032568697, 0.012707858, 0.0024554152, -0....  \n",
       "4  [-0.007715346, 0.02062087, 0.0048620435, -0.00...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom embedding using Word2Vec\n",
    "\n",
    "# Tokenize each cleaned sentence\n",
    "df['tokens'] = df['Clean_Sentence'].apply(word_tokenize)\n",
    "\n",
    "# Create a corpus: a list of token lists (one per sentence)\n",
    "corpus = df['tokens'].tolist()\n",
    "\n",
    "# Train the custom Word2Vec model on your corpus\n",
    "# vertor size is the dimension of the word vectors\n",
    "# the window size is the maximum distance between the current and predicted word within a sentence\n",
    "# min_count is the minimum count of words to consider when training the model\n",
    "# workers is the number of worker threads to train the model\n",
    "\n",
    "custom_model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, workers=4) \n",
    "\n",
    "# Function to get the average word embedding for a given sentence\n",
    "def get_custom_embedding(sentence, model):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    # Get vectors for each token in the sentence if available in the model vocabulary\n",
    "    vectors = [model.wv[token] for token in tokens if token in model.wv]\n",
    "    # Return the average of all token vectors, or a zero vector if none are found\n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)\n",
    "\n",
    "# Apply the custom embedding function to each cleaned sentence\n",
    "df['Custom_Embeddings'] = df['Clean_Sentence'].apply(lambda x: get_custom_embedding(x, custom_model))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>main_category</th>\n",
       "      <th>POS_Tags</th>\n",
       "      <th>Clean_Sentence</th>\n",
       "      <th>TF_IDF</th>\n",
       "      <th>Sentiment_Score</th>\n",
       "      <th>Pretrained_Embeddings</th>\n",
       "      <th>tokens</th>\n",
       "      <th>Custom_Embeddings</th>\n",
       "      <th>Sentence_Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>there is absolutely no question personality ef...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>[PRON, VERB, ADV, DET, NOUN, NOUN, NOUN, DET, ...</td>\n",
       "      <td>absolutely question personality effects outcom...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>[0.06916882, -0.004824684, 0.030941918, 0.0643...</td>\n",
       "      <td>[absolutely, question, personality, effects, o...</td>\n",
       "      <td>[-0.0019956154, 0.012318241, 0.0036612581, 0.0...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i cannot find year stats anyhow i needed the e...</td>\n",
       "      <td>disgust</td>\n",
       "      <td>[PRON, AUX, PART, VERB, NOUN, NOUN, ADV, PRON,...</td>\n",
       "      <td>find year stats anyhow needed excel files list...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[0.024938246, 0.053811464, 0.06385085, 0.08510...</td>\n",
       "      <td>[find, year, stats, anyhow, needed, excel, fil...</td>\n",
       "      <td>[-0.0045811804, 0.013830799, 0.0021979574, -0....</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text main_category  \\\n",
       "0  there is absolutely no question personality ef...     happiness   \n",
       "1  i cannot find year stats anyhow i needed the e...       disgust   \n",
       "\n",
       "                                            POS_Tags  \\\n",
       "0  [PRON, VERB, ADV, DET, NOUN, NOUN, NOUN, DET, ...   \n",
       "1  [PRON, AUX, PART, VERB, NOUN, NOUN, ADV, PRON,...   \n",
       "\n",
       "                                      Clean_Sentence  \\\n",
       "0  absolutely question personality effects outcom...   \n",
       "1  find year stats anyhow needed excel files list...   \n",
       "\n",
       "                                              TF_IDF  Sentiment_Score  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...            -0.05   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...             0.00   \n",
       "\n",
       "                               Pretrained_Embeddings  \\\n",
       "0  [0.06916882, -0.004824684, 0.030941918, 0.0643...   \n",
       "1  [0.024938246, 0.053811464, 0.06385085, 0.08510...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [absolutely, question, personality, effects, o...   \n",
       "1  [find, year, stats, anyhow, needed, excel, fil...   \n",
       "\n",
       "                                   Custom_Embeddings  Sentence_Length  \n",
       "0  [-0.0019956154, 0.012318241, 0.0036612581, 0.0...               12  \n",
       "1  [-0.0045811804, 0.013830799, 0.0021979574, -0....               10  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentence Length\n",
    "# used tokens to calculate the length of the sentence \n",
    "# This function tokenizes the sentence and returns the number of tokens\n",
    "# This approach is more accurate than simply counting characters or words,\n",
    "# as it considers the actual linguistic structure of the sentence.\n",
    "\n",
    "def get_sentence_length(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    return len(tokens)\n",
    "\n",
    "df['Sentence_Length'] = df['Clean_Sentence'].apply(get_sentence_length)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new excel file with the new columns\n",
    "df.to_excel('/task_4/ver_2_FINAL_DATASET_revised.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
